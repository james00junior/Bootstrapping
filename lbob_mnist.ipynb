{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Import\" the bootstrap code\n",
    "%run \"little_bootstrap\"\n",
    "%matplotlib inline\n",
    "\n",
    "# Import the necessary libraries for code in this notebook. \n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import load_diabetes, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from utils import *\n",
    "from model import *\n",
    "from config import Config\n",
    "import sys\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and testing data\n",
    "train = pd.read_csv('/Users/jamesmashiyane/Desktop/BoLb/train.csv', header=None)\n",
    "test = pd.read_csv('/Users/jamesmashiyane/Desktop/BoLb/test.csv', header=None)\n",
    "Xtrn = train.iloc[:,1]\n",
    "Ytrn = train.iloc[:,2]\n",
    "Xtest = test.iloc[:,1]\n",
    "Ytest = test.iloc[:,2]\n",
    "\n",
    "Xtrn, Ytrn, Xtest, Ytest = np.asarray(Xtrn), np.asarray(Ytrn), np.asarray(Xtest), np.asarray(Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the parameters for a grid search by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carry out the grid search, fit the model, and pring out the 'true' f1 score.\n",
    "clf_full = GridSearchCV(SVC(C=1), tuned_parameters, cv=5, n_jobs=2, scoring='f1_micro')\n",
    "clf_full.fit(Xtrn, Ytrn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_f1 = clf_full.score(Xtest, Ytest)\n",
    "print('F1 score on full data set after tuning the parameters', true_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now carry out the optimization on separate randomly generated\n",
    "# data sets of different sizes and do a bootstrap on each.\n",
    "# The histograms plotted give the bounds on estimated F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_frac in [0.2, 0.5]:\n",
    "    print('\\nRunning bootstrap on {}/10ths of the training and test data'.format(int(train_frac*10)))\n",
    "\n",
    "    X_train, _, Y_train, _ = train_test_split(Xtrn, Ytrn, test_size=train_frac, random_state=0)\n",
    "    #_, X_test, _, Y_test = train_test_split(Xtest, Ytest, test_size=train_frac, random_state=0)\n",
    "\n",
    "    clf_search = GridSearchCV(SVC(), tuned_parameters, cv=5, n_jobs=2, scoring='f1_micro')\n",
    "    clf_search.fit(X_train, Y_train)\n",
    "\n",
    "    # GridSearchCV.score does not use frequencies \n",
    "    # so it is necessary to redefine a simple \n",
    "    # SVC model.\n",
    "    clf = clf_search.best_estimator_\n",
    "\n",
    "    l = LBOB()\n",
    "    l.use_freqs = True\n",
    "    l.sample_size = 0.6\n",
    "    l.n_subsamples = 8\n",
    "    l.n_trials = 30\n",
    "    # To use the standard SVC scoring, mean accuracy\n",
    "    #l.set_score_func(lambda x, y, freq: clf.score(x, y, freq))\n",
    "    # To use an F1 score\n",
    "    l.score_func = lambda x, y, freq: metrics.f1_score(y_true=y, y_pred=clf.predict(x), sample_weight=freq, average='micro')\n",
    "    lbob_big_boot(l, Xtest, Ytest)\n",
    "    print('Independent estimate of F1 Score {:0.5}'.format(l.scores.mean()))\n",
    "    lbob_histogram(l, actual=true_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
